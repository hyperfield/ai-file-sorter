
# Consider dependencies only in project.
set(CMAKE_DEPENDS_IN_PROJECT_ONLY OFF)

# The set of languages for which implicit dependencies are needed:
set(CMAKE_DEPENDS_LANGUAGES
  )

# The set of dependency files which are needed:
set(CMAKE_DEPENDS_DEPENDENCY_FILES
  "/home/evoid/projects/ai-file-sorter/app/include/external/llama.cpp/src/llama-adapter.cpp" "llama-build/src/CMakeFiles/llama.dir/llama-adapter.cpp.o" "gcc" "llama-build/src/CMakeFiles/llama.dir/llama-adapter.cpp.o.d"
  "/home/evoid/projects/ai-file-sorter/app/include/external/llama.cpp/src/llama-arch.cpp" "llama-build/src/CMakeFiles/llama.dir/llama-arch.cpp.o" "gcc" "llama-build/src/CMakeFiles/llama.dir/llama-arch.cpp.o.d"
  "/home/evoid/projects/ai-file-sorter/app/include/external/llama.cpp/src/llama-batch.cpp" "llama-build/src/CMakeFiles/llama.dir/llama-batch.cpp.o" "gcc" "llama-build/src/CMakeFiles/llama.dir/llama-batch.cpp.o.d"
  "/home/evoid/projects/ai-file-sorter/app/include/external/llama.cpp/src/llama-chat.cpp" "llama-build/src/CMakeFiles/llama.dir/llama-chat.cpp.o" "gcc" "llama-build/src/CMakeFiles/llama.dir/llama-chat.cpp.o.d"
  "/home/evoid/projects/ai-file-sorter/app/include/external/llama.cpp/src/llama-context.cpp" "llama-build/src/CMakeFiles/llama.dir/llama-context.cpp.o" "gcc" "llama-build/src/CMakeFiles/llama.dir/llama-context.cpp.o.d"
  "/home/evoid/projects/ai-file-sorter/app/include/external/llama.cpp/src/llama-cparams.cpp" "llama-build/src/CMakeFiles/llama.dir/llama-cparams.cpp.o" "gcc" "llama-build/src/CMakeFiles/llama.dir/llama-cparams.cpp.o.d"
  "/home/evoid/projects/ai-file-sorter/app/include/external/llama.cpp/src/llama-grammar.cpp" "llama-build/src/CMakeFiles/llama.dir/llama-grammar.cpp.o" "gcc" "llama-build/src/CMakeFiles/llama.dir/llama-grammar.cpp.o.d"
  "/home/evoid/projects/ai-file-sorter/app/include/external/llama.cpp/src/llama-graph.cpp" "llama-build/src/CMakeFiles/llama.dir/llama-graph.cpp.o" "gcc" "llama-build/src/CMakeFiles/llama.dir/llama-graph.cpp.o.d"
  "/home/evoid/projects/ai-file-sorter/app/include/external/llama.cpp/src/llama-hparams.cpp" "llama-build/src/CMakeFiles/llama.dir/llama-hparams.cpp.o" "gcc" "llama-build/src/CMakeFiles/llama.dir/llama-hparams.cpp.o.d"
  "/home/evoid/projects/ai-file-sorter/app/include/external/llama.cpp/src/llama-impl.cpp" "llama-build/src/CMakeFiles/llama.dir/llama-impl.cpp.o" "gcc" "llama-build/src/CMakeFiles/llama.dir/llama-impl.cpp.o.d"
  "/home/evoid/projects/ai-file-sorter/app/include/external/llama.cpp/src/llama-io.cpp" "llama-build/src/CMakeFiles/llama.dir/llama-io.cpp.o" "gcc" "llama-build/src/CMakeFiles/llama.dir/llama-io.cpp.o.d"
  "/home/evoid/projects/ai-file-sorter/app/include/external/llama.cpp/src/llama-kv-cache-iswa.cpp" "llama-build/src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o" "gcc" "llama-build/src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o.d"
  "/home/evoid/projects/ai-file-sorter/app/include/external/llama.cpp/src/llama-kv-cache.cpp" "llama-build/src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o" "gcc" "llama-build/src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o.d"
  "/home/evoid/projects/ai-file-sorter/app/include/external/llama.cpp/src/llama-memory-hybrid.cpp" "llama-build/src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o" "gcc" "llama-build/src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o.d"
  "/home/evoid/projects/ai-file-sorter/app/include/external/llama.cpp/src/llama-memory-recurrent.cpp" "llama-build/src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o" "gcc" "llama-build/src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o.d"
  "/home/evoid/projects/ai-file-sorter/app/include/external/llama.cpp/src/llama-memory.cpp" "llama-build/src/CMakeFiles/llama.dir/llama-memory.cpp.o" "gcc" "llama-build/src/CMakeFiles/llama.dir/llama-memory.cpp.o.d"
  "/home/evoid/projects/ai-file-sorter/app/include/external/llama.cpp/src/llama-mmap.cpp" "llama-build/src/CMakeFiles/llama.dir/llama-mmap.cpp.o" "gcc" "llama-build/src/CMakeFiles/llama.dir/llama-mmap.cpp.o.d"
  "/home/evoid/projects/ai-file-sorter/app/include/external/llama.cpp/src/llama-model-loader.cpp" "llama-build/src/CMakeFiles/llama.dir/llama-model-loader.cpp.o" "gcc" "llama-build/src/CMakeFiles/llama.dir/llama-model-loader.cpp.o.d"
  "/home/evoid/projects/ai-file-sorter/app/include/external/llama.cpp/src/llama-model-saver.cpp" "llama-build/src/CMakeFiles/llama.dir/llama-model-saver.cpp.o" "gcc" "llama-build/src/CMakeFiles/llama.dir/llama-model-saver.cpp.o.d"
  "/home/evoid/projects/ai-file-sorter/app/include/external/llama.cpp/src/llama-model.cpp" "llama-build/src/CMakeFiles/llama.dir/llama-model.cpp.o" "gcc" "llama-build/src/CMakeFiles/llama.dir/llama-model.cpp.o.d"
  "/home/evoid/projects/ai-file-sorter/app/include/external/llama.cpp/src/llama-quant.cpp" "llama-build/src/CMakeFiles/llama.dir/llama-quant.cpp.o" "gcc" "llama-build/src/CMakeFiles/llama.dir/llama-quant.cpp.o.d"
  "/home/evoid/projects/ai-file-sorter/app/include/external/llama.cpp/src/llama-sampling.cpp" "llama-build/src/CMakeFiles/llama.dir/llama-sampling.cpp.o" "gcc" "llama-build/src/CMakeFiles/llama.dir/llama-sampling.cpp.o.d"
  "/home/evoid/projects/ai-file-sorter/app/include/external/llama.cpp/src/llama-vocab.cpp" "llama-build/src/CMakeFiles/llama.dir/llama-vocab.cpp.o" "gcc" "llama-build/src/CMakeFiles/llama.dir/llama-vocab.cpp.o.d"
  "/home/evoid/projects/ai-file-sorter/app/include/external/llama.cpp/src/llama.cpp" "llama-build/src/CMakeFiles/llama.dir/llama.cpp.o" "gcc" "llama-build/src/CMakeFiles/llama.dir/llama.cpp.o.d"
  "/home/evoid/projects/ai-file-sorter/app/include/external/llama.cpp/src/unicode-data.cpp" "llama-build/src/CMakeFiles/llama.dir/unicode-data.cpp.o" "gcc" "llama-build/src/CMakeFiles/llama.dir/unicode-data.cpp.o.d"
  "/home/evoid/projects/ai-file-sorter/app/include/external/llama.cpp/src/unicode.cpp" "llama-build/src/CMakeFiles/llama.dir/unicode.cpp.o" "gcc" "llama-build/src/CMakeFiles/llama.dir/unicode.cpp.o.d"
  )

# Targets to which this target links.
set(CMAKE_TARGET_LINKED_INFO_FILES
  "/home/evoid/projects/ai-file-sorter/build-tests/llama-build/ggml/src/CMakeFiles/ggml.dir/DependInfo.cmake"
  "/home/evoid/projects/ai-file-sorter/build-tests/llama-build/ggml/src/CMakeFiles/ggml-cpu.dir/DependInfo.cmake"
  "/home/evoid/projects/ai-file-sorter/build-tests/llama-build/ggml/src/CMakeFiles/ggml-base.dir/DependInfo.cmake"
  )

# Fortran module output directory.
set(CMAKE_Fortran_TARGET_MODULE_DIR "")
